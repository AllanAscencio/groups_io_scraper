{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: python-dotenv in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: selenium in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.18.1)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: trio~=0.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (0.25.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3.12 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 python-dotenv selenium pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "class GroupsIOScraper:\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        self.base_url = \"https://groups.io\"\n",
    "        self.login_url = f\"{self.base_url}/login\"\n",
    "        self.driver = None\n",
    "        \n",
    "    def login_with_requests(self, email, password):\n",
    "        \"\"\"Attempt to login using requests library\"\"\"\n",
    "        try:\n",
    "            # Get the login page first to obtain CSRF token\n",
    "            response = self.session.get(self.login_url)\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract CSRF token\n",
    "            csrf_token = soup.find('input', {'name': 'csrf'})['value']\n",
    "            monocle_token = soup.find('input', {'name': 'monocle'})['value']\n",
    "            \n",
    "            # Prepare login data\n",
    "            login_data = {\n",
    "                'email': email,\n",
    "                'password': password,\n",
    "                'csrf': csrf_token,\n",
    "                'monocle': monocle_token,\n",
    "                'timezone': 'America/New_York'\n",
    "            }\n",
    "            \n",
    "            # Attempt login\n",
    "            response = self.session.post(self.login_url, data=login_data)\n",
    "            \n",
    "            # Check if login was successful\n",
    "            if response.url != self.login_url:  # Usually redirects after successful login\n",
    "                print(\"Login successful using requests!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Login failed using requests, trying Selenium...\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during requests login: {str(e)}\")\n",
    "            return False\n",
    "            \n",
    "    def login_with_selenium(self, email, password):\n",
    "        \"\"\"Attempt to login using Selenium\"\"\"\n",
    "        try:\n",
    "            # Initialize Chrome driver if not already initialized\n",
    "            if self.driver is None:\n",
    "                self.driver = webdriver.Chrome()\n",
    "            \n",
    "            self.driver.get(self.login_url)\n",
    "            \n",
    "            # Wait for email field and enter credentials\n",
    "            email_field = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, \"email\"))\n",
    "            )\n",
    "            email_field.send_keys(email)\n",
    "            \n",
    "            # Find and fill password field\n",
    "            password_field = self.driver.find_element(By.ID, \"password\")\n",
    "            password_field.send_keys(password)\n",
    "            \n",
    "            # Click login button\n",
    "            login_button = self.driver.find_element(By.ID, \"loginbutton\")\n",
    "            login_button.click()\n",
    "            \n",
    "            # Wait for redirect or new element that indicates successful login\n",
    "            time.sleep(3)  # Give it some time to process\n",
    "            \n",
    "            # Check if login was successful\n",
    "            if self.driver.current_url != self.login_url:\n",
    "                print(\"Login successful using Selenium!\")\n",
    "                return True\n",
    "            else:\n",
    "                print(\"Login failed using Selenium\")\n",
    "                self.quit_driver()\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error during Selenium login: {str(e)}\")\n",
    "            self.quit_driver()\n",
    "            return False\n",
    "    \n",
    "    def quit_driver(self):\n",
    "        \"\"\"Safely quit the Selenium driver\"\"\"\n",
    "        if self.driver is not None:\n",
    "            self.driver.quit()\n",
    "            self.driver = None\n",
    "    \n",
    "    def get_driver(self):\n",
    "        \"\"\"Return the current driver instance\"\"\"\n",
    "        return self.driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscraper = GroupsIOScraper()\\nif scraper.login_with_selenium(email, password):\\n    forum_scraper = GroupsIOForumScraper(scraper.get_driver())\\n    posts, replies = forum_scraper.scrape_forum(max_pages=5)\\n    forum_scraper.save_to_csv()\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "class GroupsIOForumScraper:\n",
    "    def __init__(self, driver):\n",
    "        \"\"\"Initialize with Selenium webdriver instance\"\"\"\n",
    "        self.driver = driver\n",
    "        self.base_url = \"https://groups.io/g/peds-endo\"\n",
    "        self.posts_data = []\n",
    "        self.replies_data = []\n",
    "        \n",
    "    def navigate_to_topics(self):\n",
    "        \"\"\"Navigate to the topics page\"\"\"\n",
    "        try:\n",
    "            topics_url = f\"{self.base_url}/topics\"\n",
    "            self.driver.get(topics_url)\n",
    "            time.sleep(3)  # Wait for page to load\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error navigating to topics page: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def get_next_page_url(self):\n",
    "        \"\"\"Extract the URL for the next page\"\"\"\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        pagination = soup.find('ul', {'class': 'pagination'})\n",
    "        if pagination:\n",
    "            next_link = pagination.find('a', href=lambda x: x and 'page=' in x)\n",
    "            if next_link:\n",
    "                # Return complete URL\n",
    "                return f\"https://groups.io{next_link['href']}\" if next_link['href'].startswith('/') else next_link['href']\n",
    "        return None\n",
    "\n",
    "    def collect_all_preview_data(self, max_pages=None):\n",
    "        \"\"\"Collect preview data from all pages before processing individual posts\"\"\"\n",
    "        print(\"Starting to collect preview data from all pages...\")\n",
    "        all_preview_data = []\n",
    "        current_page = 1\n",
    "        \n",
    "        while True:\n",
    "            print(f\"Collecting previews from page {current_page}...\")\n",
    "            \n",
    "            # Scrape current page\n",
    "            page_topics = self.scrape_topics_page()\n",
    "            all_preview_data.extend(page_topics)\n",
    "            \n",
    "            # Check if we've reached max_pages\n",
    "            if max_pages and current_page >= max_pages:\n",
    "                print(f\"Reached maximum pages limit ({max_pages})\")\n",
    "                break\n",
    "                \n",
    "            # Get next page URL\n",
    "            next_url = self.get_next_page_url()\n",
    "            if not next_url:\n",
    "                print(\"No more pages available\")\n",
    "                break\n",
    "                \n",
    "            # Navigate to next page\n",
    "            print(f\"Navigating to next page: {next_url}\")\n",
    "            self.driver.get(next_url)\n",
    "            time.sleep(3)  # Wait for page load\n",
    "            current_page += 1\n",
    "            \n",
    "        print(f\"Collected preview data for {len(all_preview_data)} topics\")\n",
    "        return all_preview_data\n",
    "\n",
    "    def process_posts_with_replies(self, preview_data):\n",
    "        \"\"\"Process individual posts that have replies\"\"\"\n",
    "        print(\"Starting to process individual posts with replies...\")\n",
    "        \n",
    "        for topic in preview_data:\n",
    "            if topic['reply_count'] > 0:\n",
    "                print(f\"Processing post: {topic['title']} (Reply count: {topic['reply_count']})\")\n",
    "                self.scrape_single_post(topic['url'], topic.copy())\n",
    "            else:\n",
    "                # For posts without replies, just add the preview data\n",
    "                topic['post_id'] = self.extract_post_id(topic['url'])\n",
    "                topic['full_content'] = topic['preview']\n",
    "                self.posts_data.append(topic)\n",
    "\n",
    "    def scrape_topics_page(self):\n",
    "        \"\"\"Scrape all topics from the current page\"\"\"\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        topics = []\n",
    "        \n",
    "        # Find all topic rows in the table\n",
    "        topic_rows = soup.find('table', {'id': 'records'}).find_all('tr')\n",
    "        \n",
    "        for row in topic_rows:\n",
    "            topic_data = self._parse_topic_row(row)\n",
    "            if topic_data:\n",
    "                topics.append(topic_data)\n",
    "                \n",
    "        return topics\n",
    "    \n",
    "    def _parse_topic_row(self, row):\n",
    "        \"\"\"Parse individual topic row\"\"\"\n",
    "        try:\n",
    "            # Main container for topic info\n",
    "            topic_container = row.find('div', {'style': 'margin-top:3px;margin-bottom:3px;'})\n",
    "            if not topic_container:\n",
    "                return None\n",
    "                \n",
    "            # Get topic link and title\n",
    "            subject_span = topic_container.find('span', {'class': 'subject'})\n",
    "            if not subject_span:\n",
    "                return None\n",
    "                \n",
    "            link = subject_span.find('a')\n",
    "            topic_url = link['href'] if link else None\n",
    "            topic_title = link.text.strip() if link else None\n",
    "            \n",
    "            # Get topic preview\n",
    "            preview_div = topic_container.find('div', {'class': 'truncate-one-line'})\n",
    "            preview_text = preview_div.text.strip() if preview_div else None\n",
    "            \n",
    "            # Get thread info (author, dates)\n",
    "            thread_info = topic_container.find('span', {'class': 'thread-attribution'})\n",
    "            author = thread_info.find(string=True, recursive=False).strip().replace('Started by', '').strip() if thread_info else None\n",
    "            \n",
    "            # Get dates\n",
    "            dates = thread_info.find_all('span', {'title': True})\n",
    "            start_date = dates[0]['title'] if dates else None\n",
    "            last_reply_date = dates[1]['title'] if len(dates) > 1 else None\n",
    "            \n",
    "            # Get reply count\n",
    "            reply_count = 0\n",
    "            hashtag_span = subject_span.find('span', {'class': 'hashtag-position'})\n",
    "            if hashtag_span:\n",
    "                try:\n",
    "                    reply_count = int(hashtag_span.text.strip())\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                    \n",
    "            # Get if attachments exist\n",
    "            has_attachments = bool(subject_span.find('i', {'class': 'fa-paperclip'}))\n",
    "            \n",
    "            return {\n",
    "                'title': topic_title,\n",
    "                'url': topic_url,\n",
    "                'preview': preview_text,\n",
    "                'author': author,\n",
    "                'start_date': start_date,\n",
    "                'last_reply_date': last_reply_date,\n",
    "                'reply_count': reply_count,\n",
    "                'has_attachments': has_attachments\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing row: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_post_id(self, url):\n",
    "        \"\"\"Extract post ID from URL\"\"\"\n",
    "        try:\n",
    "            return url.split('/')[-1]\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    def parse_date(self, date_str):\n",
    "        \"\"\"Parse date string into standardized format\"\"\"\n",
    "        try:\n",
    "            # Add your date parsing logic here\n",
    "            return date_str\n",
    "        except:\n",
    "            return None\n",
    "            \n",
    "    def scrape_single_post(self, url, topic_data):\n",
    "        \"\"\"Scrape an individual post and its replies\"\"\"\n",
    "        try:\n",
    "            # Navigate to post page\n",
    "            self.driver.get(url)\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            \n",
    "            # Extract main post content\n",
    "            main_post = soup.find('div', {'class': 'table-background-color expanded-message'})\n",
    "            if not main_post:\n",
    "                return\n",
    "                \n",
    "            # Get main post details\n",
    "            post_id = self.extract_post_id(url)\n",
    "            author = main_post.find('u').text.strip() if main_post.find('u') else None\n",
    "            date = main_post.find('span', {'title': True})['title'] if main_post.find('span', {'title': True}) else None\n",
    "            content = main_post.find('div', {'id': lambda x: x and x.startswith('msgbody')}).text.strip() if main_post.find('div', {'id': lambda x: x and x.startswith('msgbody')}) else None\n",
    "            \n",
    "            # Update the topic data with full content\n",
    "            topic_data.update({\n",
    "                'post_id': post_id,\n",
    "                'full_content': content,\n",
    "                'author': author,\n",
    "                'date': self.parse_date(date)\n",
    "            })\n",
    "            self.posts_data.append(topic_data)\n",
    "            \n",
    "            # Get replies (all subsequent expanded-message divs)\n",
    "            replies = soup.find_all('div', {'class': 'table-background-color expanded-message'})[1:]  # Skip first one (main post)\n",
    "            \n",
    "            for reply in replies:\n",
    "                reply_data = {\n",
    "                    'parent_post_id': post_id,\n",
    "                    'reply_author': reply.find('u').text.strip() if reply.find('u') else None,\n",
    "                    'reply_date': self.parse_date(reply.find('span', {'title': True})['title']) if reply.find('span', {'title': True}) else None,\n",
    "                    'reply_content': reply.find('div', {'id': lambda x: x and x.startswith('msgbody')}).text.strip() if reply.find('div', {'id': lambda x: x and x.startswith('msgbody')}) else None\n",
    "                }\n",
    "                self.replies_data.append(reply_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping post {url}: {str(e)}\")\n",
    "            \n",
    "    def scrape_forum(self, max_pages=None):\n",
    "        \"\"\"Main method to scrape the forum\"\"\"\n",
    "        if not self.navigate_to_topics():\n",
    "            return False\n",
    "            \n",
    "        # First collect all preview data\n",
    "        preview_data = self.collect_all_preview_data(max_pages)\n",
    "        \n",
    "        # Then process posts with replies\n",
    "        self.process_posts_with_replies(preview_data)\n",
    "        \n",
    "        return self.posts_data, self.replies_data\n",
    "            \n",
    "    def save_to_csv(self, posts_filename='posts.csv', replies_filename='replies.csv'):\n",
    "        \"\"\"Save posts and replies to separate CSV files\"\"\"\n",
    "        # Save posts\n",
    "        posts_df = pd.DataFrame(self.posts_data)\n",
    "        posts_df.to_csv(posts_filename, index=False)\n",
    "        print(f\"Saved {len(self.posts_data)} posts to {posts_filename}\")\n",
    "        \n",
    "        # Save replies\n",
    "        replies_df = pd.DataFrame(self.replies_data)\n",
    "        replies_df.to_csv(replies_filename, index=False)\n",
    "        print(f\"Saved {len(self.replies_data)} replies to {replies_filename}\")\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "scraper = GroupsIOScraper()\n",
    "if scraper.login_with_selenium(email, password):\n",
    "    forum_scraper = GroupsIOForumScraper(scraper.get_driver())\n",
    "    posts, replies = forum_scraper.scrape_forum(max_pages=5)\n",
    "    forum_scraper.save_to_csv()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during requests login: 'NoneType' object is not subscriptable\n",
      "Login successful using Selenium!\n"
     ]
    }
   ],
   "source": [
    "scraper = GroupsIOScraper()\n",
    "email = \"allan.ascencio@gmail.com\"\n",
    "password = \"Sah%b5BGn9TBgia\"\n",
    "\n",
    "# Try requests first, then Selenium if needed\n",
    "if not scraper.login_with_requests(email, password):\n",
    "    selenium_success = scraper.login_with_selenium(email, password)\n",
    "    if not selenium_success:\n",
    "        print(\"All login attempts failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Use the driver for other operations\n",
    "driver = scraper.get_driver()\n",
    "if driver:\n",
    "    # Use the driver for additional operations\n",
    "    driver.get(\"https://groups.io/g/peds-endo/topics\")\n",
    "    # ... perform other operations ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful using Selenium!\n",
      "Starting to collect preview data from all pages...\n",
      "Collecting previews from page 1...\n",
      "Navigating to next page: https://groups.io/g/peds-endo/topics?page=2&after=1733530647300743547\n",
      "Collecting previews from page 2...\n",
      "Navigating to next page: https://groups.io/g/peds-endo/topics?page=3&after=1731481815837865745\n",
      "Collecting previews from page 3...\n",
      "Navigating to next page: https://groups.io/g/peds-endo/topics?page=2&before=1731366218515591380\n",
      "Collecting previews from page 4...\n",
      "Navigating to next page: https://groups.io/g/peds-endo/topics?page=3&after=1731481815837865745\n",
      "Collecting previews from page 5...\n",
      "Reached maximum pages limit (5)\n",
      "Collected preview data for 100 topics\n",
      "Starting to process individual posts with replies...\n",
      "Processing post: Ped Endo in Orlando, FL (Reply count: 2)\n",
      "Processing post: Hypercalcemia Case (Reply count: 8)\n",
      "Processing post: Thyroid- DIO2 rs225014 homozygous genotype (Reply count: 3)\n",
      "Processing post: Ped Endo in Austin TX and Valdosta GA (Reply count: 5)\n",
      "Processing post: Metabolic bone disease (Reply count: 4)\n",
      "Processing post: POI and elevated DHEAS (Reply count: 13)\n",
      "Processing post: levothyroxine absorption test (Reply count: 5)\n",
      "Processing post: NR5A1 mutation experience (Reply count: 7)\n",
      "Processing post: RD/CDCES vs RN/CDCES - Salary Question (Reply count: 2)\n",
      "Processing post: interesting case of elevated testosterone (Reply count: 4)\n",
      "Processing post: MEN-1 and small hepatic lesion (Reply count: 12)\n",
      "Processing post: Adrenal recovery after treatment with mitotane? (Reply count: 2)\n",
      "Processing post: Primary autoimmune adrenocortical insufficy expertise needed (Reply count: 3)\n",
      "Processing post: - Re: [peds-endo] Stage 1 type 1 diabetes (Reply count: 3)\n",
      "Processing post: Stage 1 type 1 diabetes (Reply count: 2)\n",
      "Processing post: patient with T1DM moving to New Zealan (Reply count: 2)\n",
      "Processing post: Two open fellowship positions at Lurie Children's Hospital of Chicago (Reply count: 2)\n",
      "Processing post: Difficult case of hypoglycemia in a kid with Trisomy 21 (Reply count: 16)\n",
      "Processing post: Ped Endo in Rio (Reply count: 2)\n",
      "Processing post: Dose of desmopressin for IPSS (Reply count: 3)\n",
      "Processing post: androgen insensitivity syndrome and craniosynostosis (Reply count: 4)\n",
      "Processing post: A Thanksgiving Message of Gratitude (Reply count: 2)\n",
      "Processing post: Bisphosphonate for Pain Relief in Metastatic Bone CA (Reply count: 3)\n",
      "Processing post: Follow up on a previous case of hypercalcemia (Reply count: 2)\n",
      "Processing post: Alstrom syndrome (Reply count: 2)\n",
      "Processing post: case : puberty (Reply count: 8)\n",
      "Processing post: - Re: [peds-endo] [case] Cushing Syndrome....no really...I think I found one! (Reply count: 4)\n",
      "Processing post: [case] Cushing Syndrome....no really...I think I found one! (Reply count: 6)\n",
      "Processing post: - [peds-endo] case : puberty (Reply count: 4)\n",
      "Processing post: Reminder: Growth hormone and brain MRI survey (Reply count: 2)\n",
      "Processing post: Panhypopituitarism secondary to craniopharyngioma resection (Reply count: 3)\n",
      "Processing post: Peds Endo near Talcahuano Chile (Reply count: 2)\n",
      "Processing post: treatment of tall stature in Weaver Syndrome (Reply count: 3)\n",
      "Processing post: SEMA3A mutation (Reply count: 4)\n",
      "Processing post: Question about Bicalutamide use in precocious puberty (Reply count: 4)\n",
      "Processing post: Case - recent change in responses to growth hormone and hCG (Reply count: 2)\n",
      "Processing post: Peds Endo positions (Reply count: 3)\n",
      "Processing post: Pedi endocrinologist at Nemours? (Reply count: 5)\n",
      "Processing post: - Re: [peds-endo] Stage 1 type 1 diabetes (Reply count: 3)\n",
      "Processing post: Stage 1 type 1 diabetes (Reply count: 2)\n",
      "Processing post: patient with T1DM moving to New Zealan (Reply count: 2)\n",
      "Processing post: Two open fellowship positions at Lurie Children's Hospital of Chicago (Reply count: 2)\n",
      "Processing post: Difficult case of hypoglycemia in a kid with Trisomy 21 (Reply count: 16)\n",
      "Processing post: Ped Endo in Rio (Reply count: 2)\n",
      "Processing post: Dose of desmopressin for IPSS (Reply count: 3)\n",
      "Processing post: androgen insensitivity syndrome and craniosynostosis (Reply count: 4)\n",
      "Processing post: A Thanksgiving Message of Gratitude (Reply count: 2)\n",
      "Processing post: Bisphosphonate for Pain Relief in Metastatic Bone CA (Reply count: 3)\n",
      "Processing post: Follow up on a previous case of hypercalcemia (Reply count: 2)\n",
      "Processing post: Alstrom syndrome (Reply count: 2)\n",
      "Processing post: case : puberty (Reply count: 8)\n",
      "Processing post: - Re: [peds-endo] [case] Cushing Syndrome....no really...I think I found one! (Reply count: 4)\n",
      "Processing post: [case] Cushing Syndrome....no really...I think I found one! (Reply count: 6)\n",
      "Processing post: - [peds-endo] case : puberty (Reply count: 4)\n",
      "Processing post: Reminder: Growth hormone and brain MRI survey (Reply count: 2)\n",
      "Processing post: Panhypopituitarism secondary to craniopharyngioma resection (Reply count: 3)\n",
      "Processing post: Peds Endo near Talcahuano Chile (Reply count: 2)\n",
      "Processing post: treatment of tall stature in Weaver Syndrome (Reply count: 3)\n",
      "Processing post: SEMA3A mutation (Reply count: 4)\n",
      "Processing post: Question about Bicalutamide use in precocious puberty (Reply count: 4)\n",
      "Processing post: Case - recent change in responses to growth hormone and hCG (Reply count: 2)\n",
      "Processing post: Peds Endo positions (Reply count: 3)\n",
      "Processing post: Pedi endocrinologist at Nemours? (Reply count: 5)\n",
      "Saved 100 posts to posts.csv\n",
      "Saved 193 replies to replies.csv\n"
     ]
    }
   ],
   "source": [
    "scraper = GroupsIOScraper()\n",
    "if scraper.login_with_selenium(email, password):\n",
    "    forum_scraper = GroupsIOForumScraper(scraper.get_driver())\n",
    "    posts, replies = forum_scraper.scrape_forum(max_pages=150)\n",
    "    forum_scraper.save_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Clean up when done\n",
    "scraper.quit_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
